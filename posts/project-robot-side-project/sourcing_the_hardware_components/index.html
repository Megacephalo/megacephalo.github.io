<!doctype html>
<html lang="en-us">
  <head>
    <title>Sourcing the hardware components // Megacephalo&#39;s Tech Blog</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.145.0">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Charly Huang" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Sourcing the hardware components">
  <meta name="twitter:description" content="To make my project a reality, I need to design and acquire the hardware to enable running the navigation algorithms and not solely relying on publicly available sensor datasets. Here are my concerns:
Functional and non-functional requirements Since the project is about developing a robot capable of navigating irregular environments either indoor or outdoor, I would like to build a robust mobile platform that is sturdy enough to traverse through slightly sloped terrain while also having enough capacity to mount two to three sensors. More specifically, my functional requirements are the following:">

    <meta property="og:url" content="https://megacephalo.github.io/posts/project-robot-side-project/sourcing_the_hardware_components/">
  <meta property="og:site_name" content="Megacephalo&#39;s Tech Blog">
  <meta property="og:title" content="Sourcing the hardware components">
  <meta property="og:description" content="To make my project a reality, I need to design and acquire the hardware to enable running the navigation algorithms and not solely relying on publicly available sensor datasets. Here are my concerns:
Functional and non-functional requirements Since the project is about developing a robot capable of navigating irregular environments either indoor or outdoor, I would like to build a robust mobile platform that is sturdy enough to traverse through slightly sloped terrain while also having enough capacity to mount two to three sensors. More specifically, my functional requirements are the following:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-27T16:35:33+08:00">
    <meta property="article:modified_time" content="2024-12-27T16:35:33+08:00">
    <meta property="article:tag" content="Robot Side-Project">


    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KR2N7R73Y8"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-KR2N7R73Y8');
    </script>
  </head>
  <body>
    <header class="app-header">
      <a href="https://megacephalo.github.io/"><img class="app-header-avatar" src="/images/avatar_headshot.png" alt="Charly Huang" /></a>
      <span class="app-header-title">Megacephalo&#39;s Tech Blog</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/posts/">All posts</a>
             - 
          
          <a class="app-header-menu-item" href="/posts/about_me">About Me</a>
             - 
          
          <a class="app-header-menu-item" href="/posts/contact_me">Contact Me</a>
      </nav>
      <p>The corner to share my learning on robotics and autonoous navigaation</p>
      <div class="app-header-social">
        
          <a href="https://github.com/Megacephalo" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://www.linkedin.com/in/charlyhuang/" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-linkedin" viewBox="0 0 24 24" fill="currentColor"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
          </a>
        
          <a href="https://www.youtube.com/" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-youtube" viewBox="0 0 24 24" fill="currentColor"><title>YouTube</title><path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Sourcing the hardware components</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Dec 27, 2024
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          11 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="https://megacephalo.github.io/tags/robot-side-project/">Robot Side-Project</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>To make my project a reality, I need to design and acquire the hardware to enable running the navigation algorithms and not solely relying on publicly available sensor datasets. Here are my concerns:</p>
<h1 id="functional-and-non-functional-requirements">Functional and non-functional requirements</h1>
<p>Since the project is about developing a robot capable of navigating irregular environments either indoor or outdoor, I would like to build a robust mobile platform that is sturdy enough to traverse through slightly sloped terrain while also having enough capacity to mount two to three sensors. More specifically, my functional requirements are the following:</p>
<h2 id="functional-requirements">Functional requirements</h2>
<h3 id="do">Do</h3>
<ul>
<li>
<p>A wheeled robot, preferably with a differential drive, Ackerman sterring, or skid-steer configuration.</p>
</li>
<li>
<p>Traverse uneven terrain with slope less than 30 degrees while maintaining 2 ~ 4 m/s speed</p>
</li>
<li>
<p>Remote controllable</p>
</li>
<li>
<p>connected via WiFi but will migrate to cellular network</p>
</li>
<li>
<p>Equip with visual sensor. The effective range should starts from 10 cm and up</p>
</li>
<li>
<p>3D perception sensor, most likely a LiDAR sensor. Its effective range should sit between 1 m to 30 m.</p>
</li>
<li>
<p>Network connection device</p>
</li>
<li>
<p>Max budget: no single component should surpass US$ 500 (Oops&hellip;3D LiDAR you are an exception, just you alone!)</p>
</li>
</ul>
<h3 id="dont">Don&rsquo;t</h3>
<ul>
<li>Not a legged robot (big doggo we will meet again, but not now&hellip;)</li>
<li>Not preferring an omnidirectional drive layout</li>
<li>Not a quadrotor or flying drone. (Mmm&hellip;this may be for my next robot project!)</li>
<li>No manipulator on top</li>
<li>Not a humanoid (yet)</li>
<li>Not a submersible or marine robot</li>
</ul>
<h2 id="non-functional-requirements">Non-functional requirements</h2>
<h3 id="do-1">Do</h3>
<ul>
<li>
<p>Should handle 2 kgs payload to navigate at least 1 hr</p>
</li>
<li>
<p>Should run several sensors simultaneously</p>
</li>
<li>
<p>Sturdy and quick-mounting housing for battery, dev kip, and sensors</p>
</li>
<li>
<p>Fit well to rover platform but is not irreversebly intrusive.</p>
</li>
</ul>
<h3 id="dont-1">Don&rsquo;t</h3>
<ul>
<li>
<p>Equip device for global positioning</p>
</li>
<li>
<p>No auto-charging connector</p>
</li>
</ul>
<hr>
<p>With the requirements in mind, I began surveying the available options in the market.</p>
<h1 id="the-robot-body-sphero-rvr">The robot body: Sphero RVR+</h1>
<p><img src="/images/project-robot-side-project/sourcing_the_hardware_components/sphero_rvr.png" alt="the Sphero RVR+ form factor"> <br>
Source: <a href="https://sphero.com/products/rvr">https://sphero.com/products/rvr</a></p>
<p>At first of my investigation, I was attracted by the fully assembled mobile rover solutions such as the <a href="https://www.u-buy.com.tw/en/product/JMLY5KY2G-hiwonder-jetson-nano-robot-voice-recognition-modeling-slam-mappingnavigation-with-7in-touchscreen-robotic-car-ai-vision-robotic-kitadvanced-kit-with?srsltid=AfmBOopwj23cKr9pUoxHR5bzxNDkVPzRQ_sdVZeM2h5QlkVfVoMkn0OZIfc&amp;ref=hm-google-redirect">HIWONDER Jetson Nano Robot Voice Recognition Modeling SLAM Mapping/Navigation with 7In Touchscreen Robotic Car AI Vision Robotic Kit (Advanced Kit with SLAMTEC A1 Lidar) </a>, <a href="https://www.ebay.com/itm/166705352196?chn=ps&amp;mkevt=1&amp;mkcid=28&amp;google_free_listing_action=view_item&amp;srsltid=AfmBOoqA0oCsu3--uISPYICBaNPkN6QSLEn9ttV_LeB929SKlTXnJ--KjK8">XIAOR GEEK XR-SLAM Lidar Robot Car with HD Camera ROS Robot Tank Car 8400Mah 12V</a>, or <a href="https://www.u-buy.com.tw/en/product/4LMLESC5E-jetson-nano-ai-robot-ros-tank-kit-transbot-with-somatosensory-depth-camera-3d-scanner-ros-robotics-coding-with-jetson-nano-4gb-sub?srsltid=AfmBOooGaCn0oZSHV-6OLHLKGn4CKTeqsmh6NjPg6BhwmD60cb-x8CZ1gzg&amp;ref=hm-google-redirect">Yahboom&rsquo;s Professional ROS Robotics Coding Kit with AI Functions and 3D Visual Mapping Navigation</a> but after closer inspection, they all have factors that do not meet my expectations. As I want to build a robot running on 3D LiDAR, these solutions could never satisft my needs. I need to source my own components as I want to build a robot that runs on 3D LiDAR instead of a 2D one.</p>
<p>Hence, I came across <a href="https://sphero.com/products/rvr">Sphero RVR+</a>, an educational rover robot for teenagers. Despite its intended users, the rover exhibits several factors that I find ideal. First, it is designed to allow the user to mount whatever components that accepts UART interface for control. More importantly, Sphero, the robot&rsquo;s manufacturer, offers a Python Public SDK to allow the user to control the rover via signal commands, which in terms offer an endless possibility to the developer to create their own applications to have the rover to collaborate with other sensors. As a developr, I can save the effort to assemble the components and ensure that my robot is stable enough for the future challenges.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/GtJXZzbfk1A?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>So with the connection-to-external development kit concern out of the way, I am also convinced that the payload capacity and the battery life can make it last long enough on a single sortie. This is the rover that I eventually enrolled in my project.</p>
<h1 id="the-brain-jetson-orin-nano">The brain: Jetson Orin Nano</h1>
<p><img src="/images/project-robot-side-project/sourcing_the_hardware_components/jetson-orin-nano-qtr_numbered.png" alt="Jetson Orin Nano overview"> <br>
Source: <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit">https://developer.nvidia.com/embeGtJXZzbfk1Added/learn/get-started-jetson-orin-nano-devkit</a></p>
<p>There are currently a couple of potential contenders in the market: Arduino, Raspberry Pi, and the Jetson series. I then gathered and tabulated their publicly available tech specs to evaluate:</p>
<table>
  <thead>
      <tr>
          <th><strong>Development Kit</strong></th>
          <th>Wattage</th>
          <th>Processing Units</th>
          <th>GPU</th>
          <th>Price (approx.)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Arduino Uno</strong></td>
          <td>0.05W</td>
          <td>16 MHz ATmega328P (8-bit)</td>
          <td>NA</td>
          <td>$25</td>
      </tr>
      <tr>
          <td><strong>Raspberry Pi 4 Model B</strong></td>
          <td>3-7.5W</td>
          <td>Quad-Core ARM Cortex-A72 (64-bit)</td>
          <td>Broadcom VideoCore VI</td>
          <td>$35</td>
      </tr>
      <tr>
          <td><strong>Raspberry Pi 5 (8 Gb)</strong></td>
          <td>2.7-15 W</td>
          <td>Broadcom BCM2712 2.4GHz quad-core 64-bit Arm Cortex-A76</td>
          <td>Broadcom VideoCore VII</td>
          <td>$80</td>
      </tr>
      <tr>
          <td><strong>Jetson Nano</strong></td>
          <td>5-10W</td>
          <td>Quad-Core ARM Cortex-A57</td>
          <td>128-core Maxwell GPU</td>
          <td>$99</td>
      </tr>
      <tr>
          <td><strong>Jetson TX2</strong></td>
          <td>7.5-15W</td>
          <td>Dual-Core NVIDIA Denver 2 + Quad-Core ARM Cortex-A57</td>
          <td>256-core NVIDIA Pascal GPU</td>
          <td>$299</td>
      </tr>
      <tr>
          <td><strong>Jetson Orin Nano</strong></td>
          <td>5-15W</td>
          <td>6-core ARM Cortex-A78AE</td>
          <td>1024-core NVIDIA Ampere GPU</td>
          <td>$199</td>
      </tr>
      <tr>
          <td><strong>NVIDIA Jetson Orin Nano Super</strong></td>
          <td>10-20W</td>
          <td>8-core ARM Cortex-A78AE</td>
          <td>1024-core NVIDIA Ampere GPU</td>
          <td>$199</td>
      </tr>
  </tbody>
</table>
<p>I selected the best candidate based on the following criteria:</p>
<ul>
<li>Wattage: 5 ~ 15W</li>
<li>Provide GPU good enough to do basic machine learning algorithms especially for object detection and possibly SLAM</li>
<li>Able to run Linux</li>
<li>Budget: less or equal to US$ 300.00</li>
</ul>
<p>In this table, Jetosn Orin Nano Super was not released yet at the time of the survey (Sept 2024). From the cost-performance point of view, and to provide a little more of computing power for future potential software applications, I chose <strong>Jetson Orin Nano</strong> after making sure that the Sphero RVR+ and other sensors&rsquo; SDKs can be executed on the development kit&rsquo;s operating system. Another good thing about this dev kit is that it already has <a href="https://developer.nvidia.com/isaac">NVidia Issac</a> ROS available to it, which makes it all the more strong candidate in terms of hardware and software eco-system support.</p>
<h1 id="the-eyes-zed-2i">The eyes: ZED 2i</h1>
<p><img src="/images/project-robot-side-project/sourcing_the_hardware_components/zed_2i.png" alt="ZED 2i"> <br>
Source <a href="https://www.stereolabs.com/en-tw/products/zed-2">https://www.stereolabs.com/en-tw/products/zed-2</a></p>
<p>For this project, I also want to have a chance to work with cameras. These sensors show a couple of advantages that I believe is indispensible for a sound robot:</p>
<ul>
<li>Camera can perceive the reach features in the environment and is essential in pose estimation as well as environmental perception</li>
<li>Camera is a mature product and there are an extensive application and studies based on this type of sensor on perception as well as pose estimation</li>
<li>RGB-D or stereo cameras can compensate for the lack of depth in 2D camera models and there the state-o-the-art visual SLAM algorithms can already yied satisfying pose estimation results when used right.</li>
</ul>
<p>I have not touched any depth camera since I graduated from college so I am particularly excited to learn what available products are currently in the market. I eventually selected the following candidates to compare based on their publicly released specs. I am also listing the acronyms so everyone can be on the same page.</p>
<table>
  <thead>
      <tr>
          <th><strong>Acronym</strong></th>
          <th><strong>Full Name</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>FoV</strong></td>
          <td>Field of View</td>
      </tr>
      <tr>
          <td><strong>FPS</strong></td>
          <td>Frames per second</td>
      </tr>
      <tr>
          <td><strong>RGB</strong></td>
          <td>Red-Green-Blue</td>
      </tr>
      <tr>
          <td><strong>IMU</strong></td>
          <td>Inertial measurement unit</td>
      </tr>
  </tbody>
</table>
<p>The comparison chart:</p>
<table>
  <thead>
      <tr>
          <th><strong>Camera model</strong></th>
          <th><strong>Astra Series</strong></th>
          <th><strong>Intel RealSense D435i</strong></th>
          <th><strong>StereoLab ZED2i</strong></th>
          <th><strong>Intel Realsense D455</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Technology</strong></td>
          <td>Structured light</td>
          <td>Stereo / rolling shutter</td>
          <td>Stereo</td>
          <td>Stereo / global shutter</td>
      </tr>
      <tr>
          <td><strong>Wavelength</strong></td>
          <td>850 nm</td>
          <td>NA</td>
          <td>NA</td>
          <td>NA</td>
      </tr>
      <tr>
          <td><strong>H FoV</strong></td>
          <td>58.4</td>
          <td>87</td>
          <td>120</td>
          <td>90</td>
      </tr>
      <tr>
          <td><strong>V FoV</strong></td>
          <td>45.5</td>
          <td>58</td>
          <td>61</td>
          <td>65</td>
      </tr>
      <tr>
          <td><strong>Minimum range</strong></td>
          <td>0.4</td>
          <td>0.28</td>
          <td>0.3</td>
          <td>0.52</td>
      </tr>
      <tr>
          <td><strong>Max range</strong></td>
          <td>8</td>
          <td>3</td>
          <td>12</td>
          <td>6</td>
      </tr>
      <tr>
          <td><strong>FPS</strong></td>
          <td>30</td>
          <td>30</td>
          <td>15, 30, 60</td>
          <td>30</td>
      </tr>
      <tr>
          <td><strong>image dimensions</strong></td>
          <td>640 x 480</td>
          <td>1920 x 1080</td>
          <td>NA</td>
          <td>1280 x 800</td>
      </tr>
      <tr>
          <td><strong>RGB Resolution</strong></td>
          <td>1280 x 720 RGB (Astra Pro Plus)</td>
          <td>2MP</td>
          <td>1280 x 720 (1depth)</td>
          <td>NA</td>
      </tr>
      <tr>
          <td><strong>Communication Interface</strong></td>
          <td>USB 2.0 power and data</td>
          <td>USB-C</td>
          <td>USB-C</td>
          <td>USB-C</td>
      </tr>
      <tr>
          <td><strong>IMU</strong></td>
          <td>NA</td>
          <td>IMU</td>
          <td>Baroscope, IMU, Magnetometer</td>
          <td>IMU</td>
      </tr>
      <tr>
          <td><strong>Price</strong></td>
          <td>$ 149,99</td>
          <td>$334</td>
          <td>$533</td>
          <td>$419</td>
      </tr>
  </tbody>
</table>
<p>Note that the data above is retrieved on September 24, 2024. The price may differ to the latest one.</p>
<p>Out of the four candidates, I found <strong>StereoLab ZED2i</strong> particularly enticing, despite the fact that it is a stereo camera. Given the intrinsic shortcomings of the stereo-cameras, I believe that I can still live with the inconvenience while making the best use of this sensor: Reliable vision under day-light conditions or indoor scenarios while also perceiving RGB-D information at long range. And one important value that this sensor brings is that its manufacturing company - StereoLab has done a really grreat job of providing out-of-the-box SDK and ROS2 packages as well as an extensive and comprehensive documentation such that I don&rsquo;t have to wrap my head around setting up the sensor from the get go. Kudos to the StereoLab team. One last point that I find product a top-notch choice is that it comes with built-in IMU, barometer, and magnetometer; it is like spending a little extra bucks to get a bundle of sensors instead of one.</p>
<h1 id="the-depth-perception-3d-lidar">The depth perception: 3D LiDAR</h1>
<p><img src="/images/project-robot-side-project/sourcing_the_hardware_components/unitree_lidar_l1.png" alt="Unitree L1 LiDAR"> <br>
Source: <a href="https://www.unitree.com/LiDAR">https://www.unitree.com/LiDAR</a></p>
<p>3D LiDAR is another sensor that I want to equip on my robot platform as I want to explore and better understand hands-on how this sensor can revolutionize the way the robot explore its environment. The advnatages of a 3D LiDAR to a depth camera is that it intrinsically provides crispy clear depth readings of its surrounding, on paper. The downside is that the price-tag of this sensor is still out of the budget of ordinary folks like you and myself. The possible options are limited in terms of the budget. You can either shop on the second hand market without guarantee of the received sensor&rsquo;s quality and maintenance condition, or purchase a brand new one knowing that you must compromise on some of the technical restrictions as the LiDAR on the cheaper end may not bring the big guns to the table - broadcasting sparser point cloud, lower motor spin rate, or lower effective range. My compromise on the specs are the following:</p>
<ul>
<li>Within acceptable budget, i.e. within US$1,000.00</li>
<li>Wattage: less or equal to 5W</li>
<li>Weight: less or equal to 250 g</li>
<li>5 ~ 10 FPS</li>
<li>20 m effective range at 90% reflectivity</li>
<li>5 cm (0.05 m) minimum distance</li>
</ul>
<p>After doing a few surveys, I decided to order a brand new 3D LiDAR from Unitree, the Unitree L1. The specs that enticed me are the following:</p>
<ul>
<li>FoV: $360^{\circ} \times 90^{\circ}$</li>
<li>5 cm (0.05 m) minimum range</li>
<li>10 m range at 90% reflectivity</li>
<li>230 g</li>
<li>Compact size: 75 x 75 x 65 mm</li>
<li>Cost: US$249.00</li>
<li>Offers a ROS2 package of its driver</li>
</ul>
<p>But the downside is also apparent after actually running it. I found the following:</p>
<ul>
<li>Scan pattern is pretty peculiar, not the one we are familiar with on most mechanical or semi-solid state LiDARs</li>
<li>Yielding very sparse point cloud on every scan</li>
<li>Despite the sensor&rsquo;s mass being evenly distributed, the sensor still shakes violently. This phenomenon may become noticeable if the robot platform on which this sensor is installed is tilted while traversing uneven terrain.</li>
</ul>
<p>There may be other issues that I may find out as I go in my develoopment, but the sensor is performing well for now. And with its peculiar scan pattern and sparse point cloud, a dedicated scan-matching algorithm most be adopted. This is an interesting topic that I will discuss in my future posts.</p>
<h1 id="the-heart-omni-20c-power-bank">The heart: Omni 20C+ power bank</h1>
<p><img src="/images/project-robot-side-project/sourcing_the_hardware_components/omnicharge-omni-20c.webp" alt="Omni 20C+"> <br>
Source: <a href="https://intl.omnicharge.co/products/omni-20-usb-c">https://intl.omnicharge.co/products/omni-20-usb-c</a></p>
<p>When looking for options on a portable lightweight, compact and versatile enough battery to supply decent power to all the mentioned components, it has become a tricky task to survey for the candidates in the current market. Firt, the battery needs to be able hold enough power and the appropriate connector interface to transfer power across the correct wire. Next, the battery should be compact enough to be placed on top of the rover which, honestly speaking, does not offer an ample real estate to plae whatever battery our heart desires. The form factor is also another factor to consider if we want to discuss the housing design as well. Last but not least, the battery weight should also be factored in as we already have around 500 g of payload coming from the sensors. In the end to my surprise, I found the battery that I have been using to power my laptop a suitable candidate. The <a href="https://intl.omnicharge.co/products/omni-20-usb-c">Omni 20C+</a> checks all the boxes. It contains enough power and connection ports to supply the dev kit and the two sensors all on its own.</p>
<p>One last thing that I want to mention is that do not take whatever ordinary USB-C to DC barrel jack cable you have at hand to try to supply power from the power bank to your dev kit. The NVidia Jetson Orin Nano requires 5V and 1 ~ 3 A for normal operation and the wattagee will sit between 15 ~ 25 W. The power cable should be capable to hold up this amount of electricity across the wire. Therefore I ordered the <a href="https://www.amazon.com/dp/B0BGFC77M6?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1">FARSENSE USB C to DC Adapter,Barrel PD Trigger Cable(3.3ft) with 10 Connector Tips,USB to DC Power Cable</a> to get the job done. Two features I like is that it offers a voltage switch to enable the user to choose which voltage the cable should handle, and that it offers switchable barrel heads so we don&rsquo;t have to worry about the caliber of the Jetson Orin Nano when connecting it to the power bank.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In this article I went through the major components that I purchased and assembled to build my own robot. The mentioned sensors work in my planned scenarios but may not be the best choices for you. With this being said, there may be one or two sensors that I want to install in the future but the camera and the LiDAR are already enough to get the job done for now. I will update this article or start a new one in case I added a new sensor to my robot.</p>
<p>There will be a series of related articles discussing the actual hardware setup and algorithms so stay tuned.</p>
<h1 id="references">References</h1>
<p>[1] <a href="https://forums.developer.nvidia.com/t/comparison-between-raspberry-pi-and-jetson-nano/212121">https://forums.developer.nvidia.com/t/comparison-between-raspberry-pi-and-jetson-nano/212121</a> <br>
[2] <a href="https://sci-hub.se/downloads/2020-08-02/c2/10.1109@HORA49412.2020.9152915.pdf">https://sci-hub.se/downloads/2020-08-02/c2/10.1109@HORA49412.2020.9152915.pdf</a> <br>
[3] <a href="https://www.cytron.io/tutorial/pi4b-vs-jetson">https://www.cytron.io/tutorial/pi4b-vs-jetson</a> <br>
[4] <a href="https://developer.nvidia.com/embedded/jetson-tx2">https://developer.nvidia.com/embedded/jetson-tx2</a> <br>
[5] <a href="https://developer.nvidia.com/embedded/faq">https://developer.nvidia.com/embedded/faq</a> <br>
[6] <a href="https://www.macnica.co.jp/en/business/semiconductor/articles/nvidia/133162/">https://www.macnica.co.jp/en/business/semiconductor/articles/nvidia/133162/</a> <br>
[7] <a href="https://www.linkedin.com/pulse/jetson-vs-raspberry-pi-arduino-robotics-nathan-george">https://www.linkedin.com/pulse/jetson-vs-raspberry-pi-arduino-robotics-nathan-george</a> <br>
[8] <a href="https://www.researchgate.net/publication/375518943_Comparative_study_on_SBC_for_AI_and_DL_implementation">https://www.researchgate.net/publication/375518943_Comparative_study_on_SBC_for_AI_and_DL_implementation</a> <br>
[9] <a href="https://kamami.pl/en/15552-jetson-development-kits">https://kamami.pl/en/15552-jetson-development-kits</a> <br>
[10] <a href="https://datasheets.raspberrypi.com/rpi5/raspberry-pi-5-product-brief.pdf">https://datasheets.raspberrypi.com/rpi5/raspberry-pi-5-product-brief.pdf</a> <br>
[11] <a href="https://shop.unitree.com/products/unitree-4d-lidar-l1?srsltid=AfmBOopHkevbtbkG1ym8WQTt7Es-bhQycoXE1cgyjWOO_71ygBY1o2Tl">https://shop.unitree.com/products/unitree-4d-lidar-l1?srsltid=AfmBOopHkevbtbkG1ym8WQTt7Es-bhQycoXE1cgyjWOO_71ygBY1o2Tl</a> <br>
[12] <a href="https://static.generation-robots.com/media/stereolabs-zed-2i-datasheet.pdf">https://static.generation-robots.com/media/stereolabs-zed-2i-datasheet.pdf</a></p>

    </div>
    <div class="post-footer">
      

      <script src="https://utteranc.es/client.js"
        repo="megacephalo/megacephalo.github.io"
        issue-term="pathname"
        theme="github-dark"
        crossorigin="anonymous"
        async>
      </script>
    </div>
  </article>

    </main>
  </body>
</html>
